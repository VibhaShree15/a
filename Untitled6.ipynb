{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1a"
      ],
      "metadata": {
        "id": "Nt6R9UeLTU0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "import re\n",
        "\n",
        "def extract_twitter_handles(text):\n",
        "    # Define regex to match Twitter URLs with valid handles\n",
        "    pattern = r\"https://twitter\\.com/([a-zA-Z0-9_]+)\"\n",
        "    # Find all matching handles\n",
        "    handles = re.findall(pattern, text)\n",
        "    return handles\n",
        "\n",
        "# Example text\n",
        "text = \"\"\"\n",
        "Follow us on Twitter:\n",
        "1. https://twitter.com/example_handle\n",
        "2. https://twitter.com/User123\n",
        "3. https://twitter.com/invalid-handle (not valid)\n",
        "4. https://twitter.com/__valid__123\n",
        "\"\"\"\n",
        "\n",
        "# Extract and print handles\n",
        "twitter_handles = extract_twitter_handles(text)\n",
        "print(\"Extracted Twitter Handles:\", twitter_handles)\n"
      ],
      "metadata": {
        "id": "3QV6JsySTdsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1b"
      ],
      "metadata": {
        "id": "1sU2ZWl3Tgig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "id": "Y5A3DyECThYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_tokens = [ps.stem(word) for word in filtered_tokens]\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Example text\n",
        "text = \"This is an example sentence for preprocessing, which includes tokenization, stop word removal, and stemming.\"\n",
        "\n",
        "# Preprocess and print results\n",
        "processed_text = preprocess_text(text)\n",
        "print(\"Processed Text:\", processed_text)\n"
      ],
      "metadata": {
        "id": "Wiaap2vaTjga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2a"
      ],
      "metadata": {
        "id": "d-Z7X2ZmTlat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install matplotlib nltk\n"
      ],
      "metadata": {
        "id": "XZKAX-FZTmK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def plot_most_frequent_words(text, top_n=10):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "    # Count word frequencies\n",
        "    word_counts = Counter(filtered_tokens)\n",
        "\n",
        "    # Get the most common words\n",
        "    most_common_words = word_counts.most_common(top_n)\n",
        "    words, counts = zip(*most_common_words)\n",
        "\n",
        "    # Plot the graph\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(words, counts, color='skyblue')\n",
        "    plt.title(f\"Top {top_n} Most Frequent Words\", fontsize=16)\n",
        "    plt.xlabel(\"Words\", fontsize=12)\n",
        "    plt.ylabel(\"Frequency\", fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Example text\n",
        "text = \"\"\"\n",
        "This is a sample text with several words. This text is intended to test the frequency of different words\n",
        "and how often they appear in this text. Words like 'text' and 'words' appear multiple times.\n",
        "\"\"\"\n",
        "\n",
        "# Plot the graph\n",
        "plot_most_frequent_words(text, top_n=5)\n"
      ],
      "metadata": {
        "id": "CLi7OLTxToxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2b"
      ],
      "metadata": {
        "id": "In07HPs_TtFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "def compare_text_splitting_methods(text):\n",
        "    # Method 1: Using word_tokenize from NLTK\n",
        "    nltk_tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Method 2: Using Python's built-in split()\n",
        "    split_tokens = text.split()\n",
        "\n",
        "    # Method 3: Using Regular Expressions\n",
        "    regex_tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "\n",
        "    # Display results\n",
        "    print(\"Method 1: word_tokenize (NLTK)\")\n",
        "    print(nltk_tokens, \"\\n\")\n",
        "\n",
        "    print(\"Method 2: Python's split()\")\n",
        "    print(split_tokens, \"\\n\")\n",
        "\n",
        "    print(\"Method 3: Regex Splitting\")\n",
        "    print(regex_tokens, \"\\n\")\n",
        "\n",
        "    # Analyze Differences\n",
        "    print(\"Analysis:\")\n",
        "    print(f\"Number of tokens: NLTK = {len(nltk_tokens)}, split() = {len(split_tokens)}, regex = {len(regex_tokens)}\")\n",
        "\n",
        "# Example text\n",
        "text = \"This is an example sentence for splitting, including punctuation like commas, periods, and hyphensâ€”observe the difference!\"\n",
        "\n",
        "# Compare the methods\n",
        "compare_text_splitting_methods(text)\n"
      ],
      "metadata": {
        "id": "D6kuZlc7TtmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3a"
      ],
      "metadata": {
        "id": "195uIPHSTwF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def process_and_draw_pos_tags(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Generate Part-of-Speech tags\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "    # Generate Named Entity Recognition (NER) tree\n",
        "    ner_tree = ne_chunk(pos_tags)\n",
        "\n",
        "    # Draw the parse tree\n",
        "    ner_tree.draw()\n",
        "\n",
        "# Example text\n",
        "text = \"Elon Musk founded SpaceX in 2002 and Tesla in 2003.\"\n",
        "\n",
        "# Process and visualize\n",
        "process_and_draw_pos_tags(text)\n"
      ],
      "metadata": {
        "id": "w6RJxKibTwkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3b"
      ],
      "metadata": {
        "id": "jQTs79TGTzGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "import re\n",
        "\n",
        "def extract_personal_information(text):\n",
        "    # Extract name\n",
        "    name = re.search(r\"Born\\s(.+)\\n\", text)\n",
        "    name = name.group(1) if name else \"Not found\"\n",
        "\n",
        "    # Extract age\n",
        "    age = re.search(r\"\\(age\\s(\\d+)\\)\", text)\n",
        "    age = age.group(1) if age else \"Not found\"\n",
        "\n",
        "    # Extract date of birth\n",
        "    dob = re.search(r\"Born.*\\n(.+)\\(\", text)\n",
        "    dob = dob.group(1).strip() if dob else \"Not found\"\n",
        "\n",
        "    # Extract place of birth\n",
        "    pob = re.search(r\"\\)\\n(.+),\\sSouth Africa\", text)\n",
        "    pob = pob.group(1).strip() if pob else \"Not found\"\n",
        "\n",
        "    # Extract education\n",
        "    education = re.search(r\"Education\\s(.+)\\n\", text)\n",
        "    education = education.group(1) if education else \"Not found\"\n",
        "\n",
        "    # Extract all titles\n",
        "    titles = re.findall(r\"Title\\s(.+)\\n\", text)\n",
        "    titles = titles if titles else [\"Not found\"]\n",
        "\n",
        "    # Print extracted information\n",
        "    print(f\"Name: {name}\")\n",
        "    print(f\"Age: {age}\")\n",
        "    print(f\"Date of Birth: {dob}\")\n",
        "    print(f\"Place of Birth: {pob}\")\n",
        "    print(f\"Education: {education}\")\n",
        "    print(f\"Titles:\")\n",
        "    for title in titles:\n",
        "        print(f\" - {title}\")\n",
        "\n",
        "# Example text\n",
        "text = ''' Born Elon Reeve Musk\n",
        "June 28, 1971 (age 50)\n",
        "Pretoria, Transvaal, South Africa Citizenship\n",
        "South Africa\n",
        "Education University of Pennsylvania (BS, BA)\n",
        "Title Founder, CEO and Chief Engineer of SpaceX\n",
        "CEO and product architect of Tesla, Inc.\n",
        "Founder of The Boring Company and X.com (now part of PayPal)\n",
        "Co-founder of Neuralink, OpenAI, and Zip2\n",
        "Spouse(s) Justine Wilson (m. 2000; div. 2008) '''\n",
        "\n",
        "# Extract personal information\n",
        "extract_personal_information(text)\n"
      ],
      "metadata": {
        "id": "D3NvVHLOTzzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4a"
      ],
      "metadata": {
        "id": "XkRnoWKpT2xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "import string\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "    # Generate n-grams\n",
        "    n_grams = list(ngrams(filtered_tokens, n))\n",
        "    return n_grams\n",
        "\n",
        "# Example text\n",
        "text = \"Artificial intelligence has made significant advancements, but it still struggles to understand human emotions and context, limiting its ability to interact naturally.\"\n",
        "\n",
        "# Generate bigrams (n=2) and trigrams (n=3)\n",
        "bigrams = generate_ngrams(text, 2)\n",
        "trigrams = generate_ngrams(text, 3)\n",
        "\n",
        "# Print the results\n",
        "print(\"Bigrams:\", bigrams)\n",
        "print(\"Trigrams:\", trigrams)\n"
      ],
      "metadata": {
        "id": "qgOSl1v8T3dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4b"
      ],
      "metadata": {
        "id": "JNTGIOKYT6j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def plot_most_frequent_words_without_stopwords(text, top_n=10):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "    # Count word frequencies\n",
        "    word_counts = Counter(filtered_tokens)\n",
        "\n",
        "    # Get the most common words\n",
        "    most_common_words = word_counts.most_common(top_n)\n",
        "    words, counts = zip(*most_common_words)\n",
        "\n",
        "    # Plot the graph\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(words, counts, color='orange')\n",
        "    plt.title(f\"Top {top_n} Most Frequent Words (Excluding Stop Words)\", fontsize=16)\n",
        "    plt.xlabel(\"Words\", fontsize=12)\n",
        "    plt.ylabel(\"Frequency\", fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Example text\n",
        "text = \"Artificial intelligence has made significant advancements, but it still struggles to understand human emotions and context, limiting its ability to interact naturally.\"\n",
        "\n",
        "# Plot the graph\n",
        "plot_most_frequent_words_without_stopwords(text, top_n=5)\n"
      ],
      "metadata": {
        "id": "zw8Jl3LkT8Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5a"
      ],
      "metadata": {
        "id": "wU3MgwlOT9tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def pos_to_wordnet(tag):\n",
        "    # Map NLTK POS tags to WordNet POS tags\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def morphological_analysis_and_lemmatization(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Get POS tags\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Perform lemmatization based on POS tags\n",
        "    lemmatized_words = []\n",
        "    for word, tag in pos_tags:\n",
        "        wn_tag = pos_to_wordnet(tag)\n",
        "        if wn_tag:\n",
        "            lemmatized_words.append(lemmatizer.lemmatize(word, pos=wn_tag))\n",
        "        else:\n",
        "            lemmatized_words.append(word)  # If no mapping, keep the original word\n",
        "\n",
        "    # Display results\n",
        "    print(\"Original Tokens:\", tokens)\n",
        "    print(\"POS Tags:\", pos_tags)\n",
        "    print(\"Lemmatized Words:\", lemmatized_words)\n",
        "\n",
        "# Example text\n",
        "text = \"The quick brown foxes were jumping over the lazy dogs in the garden.\"\n",
        "\n",
        "# Perform morphological analysis and lemmatization\n",
        "morphological_analysis_and_lemmatization(text)\n"
      ],
      "metadata": {
        "id": "0jIlKIiJT-Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5b"
      ],
      "metadata": {
        "id": "T4K7aPpdUBEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def generate_pos_tags(sentence):\n",
        "    # Tokenize the sentence\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    # Generate POS tags\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Display results\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Generate POS tags\n",
        "generate_pos_tags(sentence)\n"
      ],
      "metadata": {
        "id": "93TB7Ai8UBwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6a"
      ],
      "metadata": {
        "id": "-GPe_x5RUEFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def analyze_stemmers_and_lemmatizer(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Initialize stemmers and lemmatizer\n",
        "    porter = PorterStemmer()\n",
        "    lancaster = LancasterStemmer()\n",
        "    snowball = SnowballStemmer(\"english\")\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Analyze each word\n",
        "    print(f\"{'Word':<15}{'PorterStemmer':<20}{'LancasterStemmer':<20}{'SnowballStemmer':<20}{'Lemmatizer':<20}\")\n",
        "    print(\"-\" * 80)\n",
        "    for word in tokens:\n",
        "        if word.isalnum():  # Skip punctuation\n",
        "            porter_stem = porter.stem(word)\n",
        "            lancaster_stem = lancaster.stem(word)\n",
        "            snowball_stem = snowball.stem(word)\n",
        "            lemma = lemmatizer.lemmatize(word)\n",
        "            print(f\"{word:<15}{porter_stem:<20}{lancaster_stem:<20}{snowball_stem:<20}{lemma:<20}\")\n",
        "\n",
        "# Example text\n",
        "text = \"The quick brown foxes were jumping over the lazy dogs in the garden.\"\n",
        "\n",
        "# Analyze stemmers and lemmatizer\n",
        "analyze_stemmers_and_lemmatizer(text)\n"
      ],
      "metadata": {
        "id": "OewQ54mDUGMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6b"
      ],
      "metadata": {
        "id": "y39bXf3kUQET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "g5RKy976UI97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "vStryC1CULT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "import spacy\n",
        "\n",
        "def extract_syntactic_dependency(text):\n",
        "    # Load the SpaCy model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Process the text\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract and print dependency information\n",
        "    print(f\"{'Token':<15}{'Dependency':<15}{'Head':<15}{'Children':<40}\")\n",
        "    print(\"-\" * 80)\n",
        "    for token in doc:\n",
        "        children = [child.text for child in token.children]\n",
        "        print(f\"{token.text:<15}{token.dep_:<15}{token.head.text:<15}{', '.join(children):<40}\")\n",
        "\n",
        "    # Visualize the dependency parse (requires Jupyter or IPython)\n",
        "    spacy.displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "\n",
        "# Example text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Extract syntactic dependency information\n",
        "extract_syntactic_dependency(text)\n"
      ],
      "metadata": {
        "id": "zUjrtD6vUNme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7"
      ],
      "metadata": {
        "id": "v1k4se8BUSaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "import nltk\n",
        "from nltk import bigrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define the collection of 3 documents\n",
        "documents = [\n",
        "    \"Artificial intelligence is transforming the world of technology.\",\n",
        "    \"Machine learning and deep learning are subsets of artificial intelligence.\",\n",
        "    \"The applications of AI in healthcare and education are remarkable.\"\n",
        "]\n",
        "\n",
        "# Tokenize and process the documents\n",
        "all_bigrams = []\n",
        "for doc in documents:\n",
        "    tokens = word_tokenize(doc.lower())  # Tokenize and convert to lowercase\n",
        "    doc_bigrams = bigrams(tokens)       # Generate bi-grams\n",
        "    all_bigrams.extend(doc_bigrams)     # Add bi-grams to the collection\n",
        "\n",
        "# Count the frequency of bi-grams\n",
        "bigram_counts = Counter(all_bigrams)\n",
        "\n",
        "# Display the total number of bi-grams\n",
        "print(f\"Total number of unique bi-grams: {len(bigram_counts)}\")\n",
        "\n",
        "# Display the top 5 most common bi-grams\n",
        "print(\"Top 5 most common bi-grams:\")\n",
        "for bigram, freq in bigram_counts.most_common(5):\n",
        "    print(f\"{bigram}: {freq}\")\n"
      ],
      "metadata": {
        "id": "W4_sy7FrUTBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8"
      ],
      "metadata": {
        "id": "QHd4G-r1UXXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#8\n",
        "import nltk\n",
        "from nltk import bigrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "def bigram_with_laplace_smoothing(corpus):\n",
        "    # Tokenize and process the corpus\n",
        "    tokens = word_tokenize(corpus.lower())  # Tokenize and convert to lowercase\n",
        "\n",
        "    # Count unigrams and bigrams\n",
        "    unigram_counts = Counter(tokens)\n",
        "    bigram_counts = Counter(bigrams(tokens))\n",
        "\n",
        "    # Vocabulary size (unique words)\n",
        "    vocab_size = len(unigram_counts)\n",
        "\n",
        "    # Define a function to calculate bigram probability with Laplace smoothing\n",
        "    def bigram_probability(word1, word2):\n",
        "        bigram = (word1, word2)\n",
        "        count_bigram = bigram_counts[bigram]\n",
        "        count_unigram = unigram_counts[word1]\n",
        "        # Add-one smoothing\n",
        "        return (count_bigram + 1) / (count_unigram + vocab_size)\n",
        "\n",
        "    # Calculate bigram probabilities\n",
        "    print(\"Bigram probabilities with Laplace smoothing:\")\n",
        "    for (word1, word2), count in bigram_counts.items():\n",
        "        prob = bigram_probability(word1, word2)\n",
        "        print(f\"P({word2} | {word1}) = {prob:.4f}\")\n",
        "\n",
        "    # Handle unseen bigrams\n",
        "    unseen_word1, unseen_word2 = \"artificial\", \"intelligence\"\n",
        "    unseen_prob = bigram_probability(unseen_word1, unseen_word2)\n",
        "    print(f\"\\nProbability of unseen bigram ('{unseen_word1}', '{unseen_word2}'): {unseen_prob:.4f}\")\n",
        "\n",
        "# Example corpus\n",
        "corpus = \"\"\"\n",
        "Artificial intelligence is transforming the world of technology.\n",
        "Machine learning and deep learning are subsets of artificial intelligence.\n",
        "The applications of AI in healthcare and education are remarkable.\n",
        "\"\"\"\n",
        "\n",
        "# Build bigram model with Laplace smoothing\n",
        "bigram_with_laplace_smoothing(corpus)\n"
      ],
      "metadata": {
        "id": "dplZNZ84UYHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10"
      ],
      "metadata": {
        "id": "XQl4IMpUUdnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#10\n",
        "def min_edit_distance(source, target):\n",
        "    # Initialize the DP table with size (len(source) + 1) x (len(target) + 1)\n",
        "    m, n = len(source), len(target)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Fill the DP table\n",
        "    for i in range(m + 1):\n",
        "        for j in range(n + 1):\n",
        "            # If source string is empty, the only option is to insert all characters of target string\n",
        "            if i == 0:\n",
        "                dp[i][j] = j\n",
        "            # If target string is empty, the only option is to delete all characters of source string\n",
        "            elif j == 0:\n",
        "                dp[i][j] = i\n",
        "            # If the characters match, no operation is needed, carry forward the value from the diagonal\n",
        "            elif source[i - 1] == target[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                # Calculate the minimum of insertion, deletion, and substitution\n",
        "                dp[i][j] = min(dp[i - 1][j - 1], dp[i][j - 1], dp[i - 1][j]) + 1\n",
        "\n",
        "    # The final answer will be in the bottom-right corner of the DP table\n",
        "    return dp[m][n]\n",
        "\n",
        "# Input source and target strings\n",
        "source_string = \"intention\"\n",
        "target_string = \"execution\"\n",
        "\n",
        "# Calculate the minimum edit distance\n",
        "distance = min_edit_distance(source_string, target_string)\n",
        "\n",
        "# Display the result\n",
        "print(f\"Minimum edit distance between '{source_string}' and '{target_string}' is: {distance}\")\n"
      ],
      "metadata": {
        "id": "g2643ZguUh_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11"
      ],
      "metadata": {
        "id": "bUEJey6FUir8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#11\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_nouns_and_adjectives(sentence):\n",
        "    # Tokenize and POS tagging\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged = pos_tag(words)\n",
        "\n",
        "    # Extract nouns (NN, NNS, NNP, NNPS) and adjectives (JJ, JJR, JJS)\n",
        "    nouns = [word for word, tag in tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
        "    adjectives = [word for word, tag in tagged if tag in ['JJ', 'JJR', 'JJS']]\n",
        "\n",
        "    return nouns, adjectives\n",
        "\n",
        "def calculate_sentiment(adjectives):\n",
        "    # Initialize Sentiment Intensity Analyzer\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "    # Calculate sentiment score for each adjective and compute the average sentiment score\n",
        "    total_score = 0\n",
        "    for adj in adjectives:\n",
        "        score = sid.polarity_scores(adj)['compound']\n",
        "        total_score += score\n",
        "\n",
        "    # Calculate average sentiment score\n",
        "    avg_sentiment_score = total_score / len(adjectives) if adjectives else 0\n",
        "    return avg_sentiment_score\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The quick brown fox jumped over the lazy dog, but the angry dog barked loudly.\"\n",
        "\n",
        "# Extract nouns and adjectives\n",
        "nouns, adjectives = extract_nouns_and_adjectives(sentence)\n",
        "\n",
        "# Calculate sentiment score of adjectives\n",
        "sentiment_score = calculate_sentiment(adjectives)\n",
        "\n",
        "# Display extracted nouns, adjectives, and sentiment score\n",
        "print(\"Nouns:\", nouns)\n",
        "print(\"Adjectives:\", adjectives)\n",
        "print(f\"Average Sentiment Score of Adjectives: {sentiment_score:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "epwBrfK5UjUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12"
      ],
      "metadata": {
        "id": "6Fq_eH6sUmv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#12\n",
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"vaccination_tweets.csv\"  # Update with your dataset file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Perform sentiment analysis\n",
        "def get_sentiment(tweet):\n",
        "    score = sid.polarity_scores(tweet)['compound']\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Apply sentiment analysis to the \"text\" column\n",
        "df['sentiment'] = df['text'].apply(get_sentiment)\n",
        "\n",
        "# Display sentiment counts\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# Plot sentiment distribution\n",
        "df['sentiment'].value_counts().plot(kind='bar', color=['green', 'red', 'gray'])\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Tweet Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0pxNnDfUUnOL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}